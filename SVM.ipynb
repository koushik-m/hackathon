{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweets = pd.read_csv('tweets_new_formatted.csv')\n",
    "\n",
    "tweets.dtypes.index\n",
    "tweets = tweets[['text', 'tweet_class', 'query']]\n",
    "\n",
    "smallest_len = min(len(tweets[tweets['tweet_class'] == 1]),\n",
    "                len( tweets[tweets['tweet_class'] == -1]),\n",
    "                len( tweets[tweets['tweet_class'] == 0]),\n",
    "               len( tweets[tweets['tweet_class'] == 2]),\n",
    "                  )\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "data = data.append(tweets[tweets['tweet_class']==-1])\n",
    "data = data.append(tweets[tweets['tweet_class']==0][:smallest_len])\n",
    "data = data.append(tweets[tweets['tweet_class']==1][:smallest_len])\n",
    "data = data.append(tweets[tweets['tweet_class']==2][:smallest_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1969, 1249) (1969,)\n",
      "0          <SHOW_NAME> top 20 villains <URL> … television\n",
      "1                                 my thoughts exactly aha\n",
      "2       <SHOW_NAME> is real sinu sou eu pic.twitter.co...\n",
      "3                                    nop fear <SHOW_NAME>\n",
      "4       novo teaser de fear <SHOW_NAME> destacando a m...\n",
      "5       freeaugmentedrealityapps arapps <SHOW_NAME>  o...\n",
      "6                                             <SHOW_NAME>\n",
      "7       me gustó un video de <HANDLE> <URL> why people...\n",
      "8       i should’ve taken some pics out in the concour...\n",
      "9                                     <SHOW_NAME> <URL> …\n",
      "10      <SHOW_NAME> <SHOW_NAME> <SHOW_NAME> <SHOW_NAME...\n",
      "11      encore un peu de fantaisie s08e11 of <SHOW_NAM...\n",
      "12      i liked a <HANDLE> video <URL> why people stop...\n",
      "13                             parece <SHOW_NAME> <URL> …\n",
      "14                                 assistindo <SHOW_NAME>\n",
      "15      <SHOW_NAME>  season 33 cap 155 catalufonia har...\n",
      "16      it only took me a year but i’ve finally finish...\n",
      "17      saturday is the best day 22  urban decay podca...\n",
      "18      young people were walking away from the party ...\n",
      "19      <SHOW_NAME> gotta be the dumbest show ever nig...\n",
      "20      <HANDLE> <HANDLE> make <SHOW_NAME> decent agai...\n",
      "21                   nowwatching <SHOW_NAME> s6 ep 15 twd\n",
      "22                     i think maybe <SHOW_NAME> season 1\n",
      "23      i suppose even the walking brain dead need a r...\n",
      "24      dead man walking the mudbugs are spot on today...\n",
      "25                   watching <SHOW_NAME> season 8 series\n",
      "26                                   no problem good luck\n",
      "27                             <SHOW_NAME> is pretty gord\n",
      "28      [comic spoilers] i just realized that the only...\n",
      "29      newgame inthe making  check out zombie apocaly...\n",
      "                              ...                        \n",
      "1939                              mimosas and <SHOW_NAME>\n",
      "1940    ive seen too many episodes of <SHOW_NAME> to t...\n",
      "1941             <SHOW_NAME>  designated survivor <URL> …\n",
      "1942    really upset that i gotta wait a whole year to...\n",
      "1943                cant wait for <SHOW_NAME> last season\n",
      "1944    i’m on season 2 of <SHOW_NAME> and i’m still n...\n",
      "1945    author george r.r martin hasn’t managed to fin...\n",
      "1946    getting real deep into <SHOW_NAME> today and i...\n",
      "1947    almost done with the 1st <SHOW_NAME> book the ...\n",
      "1948    cant choose one so my favorites <SHOW_NAME> st...\n",
      "1949    do not miss this website best of high quality ...\n",
      "1950    fucking bastards lol <SHOW_NAME> pic.twitter.c...\n",
      "1951    today stunning sansa stark from <SHOW_NAME>  m...\n",
      "1952    i’m going to watch my first episode ever of <S...\n",
      "1953    <HANDLE> <HANDLE> only way to celebrate my nam...\n",
      "1954                          provavelmente e <SHOW_NAME>\n",
      "1955                                          <SHOW_NAME>\n",
      "1956    1st look <SHOW_NAME> viserion cosplay <URL> vi...\n",
      "1957               skins true blood e <SHOW_NAME> <URL> …\n",
      "1958    the dragon and the wolf  <SHOW_NAME>  s7e7 <UR...\n",
      "1959    <URL> ◘ <SHOW_NAME> journal pic.twitter.com/r5...\n",
      "1960    looking for ward to the final two series of <H...\n",
      "1961    theres a brilliant new documentary about the <...\n",
      "1962         <SHOW_NAME> lifes lessons <URL> via <HANDLE>\n",
      "1963    george r r martin plotting <SHOW_NAME> got7 # ...\n",
      "1964    theres a brilliant new documentary about the <...\n",
      "1965    7seconds is the blacklivesmatter version of <S...\n",
      "1966    <SHOW_NAME> s05e05 online free on watchmoviesf...\n",
      "1967    that alongside the leia mary poppins moment an...\n",
      "1968    i love this photo of my husband and i at bathc...\n",
      "Name: text, Length: 1969, dtype: object\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, \n",
    "     max_df=0.95, \n",
    "     sublinear_tf = True,\n",
    "     use_idf = True,\n",
    "     ngram_range=(1, 2),\n",
    "    )\n",
    "\n",
    "classifier = LinearSVC(C=0.1)\n",
    "Xs = vectorizer.fit_transform(tweets['text'])\n",
    "\n",
    "print(Xs.shape, tweets['tweet_class'].shape)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(Xs, tweets['tweet_class'], test_size = 0.33, random_state = 42)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "# score = cross_val_score(classifier, Xs, tweets['tweet_class'], cv=2, n_jobs=-1)\n",
    "\n",
    "# print(sum(score) / len(score))\n",
    "\n",
    "print(tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6276923076923077"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy_score(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(vectorizer.transform(['it’s really a shame  <handle> '.lower()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
